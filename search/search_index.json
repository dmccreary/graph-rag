{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Learning Graph RAG","text":"<p>Welcome to our website for people learning about the powerful Graph-RAG pattern. Graph-RAG combines the best features of Generative AI, powerful large-language models and the precision of graph databases to help you build conversational agents that have a deep understanding of both documents and data. </p> <p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"glossary/","title":"Graph RAG Glossary","text":""},{"location":"glossary/#abstract-concepts","title":"Abstract Concepts","text":"<p>An idea, quality, or notion that exists in thought or as an intellectual construct rather than as a physical or tangible object.</p> <p>Unlike concrete concepts, which refer to things that can be perceived through the senses (such as \u201ctree\u201d or \u201ccar\u201d), abstract concepts represent intangible entities like emotions, relationships, moral values, theories, and principles. Examples of abstract concepts include ideas like \u201cfreedom,\u201d \u201cjustice,\u201d \u201chappiness,\u201d and \u201clove.\u201d These concepts often require a deeper level of cognitive processing and are critical in understanding complex, philosophical, or theoretical ideas.</p> <p>Examples of Abstract Concepts:</p> <ol> <li>Emotions and Feeling: happiness, sorrow, anger, pain</li> <li>Qualities: honesty, bravery, intelligence</li> <li>Theories: evolution, relativity</li> <li>Values: freedom, justice, equality</li> <li>States: peace, chaos, order</li> </ol> <p>Even small language models are good finding concrete entities.  However, there is additional processing time and cost associated with extracting abstract concepts from text and inserting them into a graph.  Qualities may also be inferred from adjectives that describe a person such as \"an honest person\".</p>"},{"location":"glossary/#community-detection","title":"Community Detection","text":"<p>Community detection is the process of identifying groups of nodes within a network that are more densely connected to each other than to the rest of the network. This technique is used to reveal the modular structure of networks, uncovering clusters or communities that share common properties or functions.</p> <p>For example, in social network analysis, community detection can help identify groups of users who frequently interact with each other, such as friend circles or interest groups.</p>"},{"location":"glossary/#concept-network","title":"Concept Network","text":"<p>A structured representation of related terms and concepts that captures the relationships between them, such as synonyms, alternate terms, abbreviations, broader terms, and narrower terms.</p> <p>Concept networks serve as a semantic map that links various expressions of an idea or entity, enabling the system to recognize and interpret different linguistic variations of the same concept or to bind a concept with opposite terms.</p>"},{"location":"glossary/#document-chunking","title":"Document Chunking","text":"<p>Document chunking is the process of dividing a large document into smaller, manageable sections or chunks. This technique is commonly used in natural language processing to facilitate tasks such as information retrieval, summarization, or content analysis by processing these smaller chunks individually.</p> <p>For example, in a question-answering system, document chunking enables the model to focus on specific sections of a document, improving accuracy and efficiency in finding relevant answers.</p>"},{"location":"glossary/#complex-sequential-question-answering-csqa","title":"Complex Sequential Question Answering (CSQA)","text":"<p>Complex Sequential Question Answering refers to a task in natural language processing that involves answering a series of interrelated questions where the context or the answers to previous questions impact subsequent questions. This task requires handling complex dependencies and reasoning chains, often relying on structured knowledge bases or graphs to provide accurate and context-aware responses. In GraphRAG, CSQA tasks benefit from the ability to retrieve and reason over graph-structured data that captures these intricate relationships\u200b(GraphRAG-Survey).#### Entity Extraction</p>"},{"location":"glossary/#concrete-entities","title":"Concrete Entities","text":"<p>Entities the correspond to physical items in the world.  These are often referred to as POLE entities (People, Organizations, Locations and Events).  These are often proper names and it is usually easy for even small language models to identify them.</p> <p>Concrete entities are often contrasted with abstract [Abstract Concepts] </p>"},{"location":"glossary/#entity-extraction","title":"Entity Extraction","text":"<p>The process of identifying and categorizing key pieces of information (entities) from text, such as names of people, organizations, locations, events, and other concepts.</p> <p>These entities are often represented by proper nouns or specific phrases that are crucial to understanding the text's meaning.</p>"},{"location":"glossary/#gnn-based-retriever","title":"GNN-based Retriever","text":"<p>GNN-based retrievers use Graph Neural Networks to understand and process graph structures. They encode graph data, score different retrieval granularities based on similarity to the query, and guide the retrieval process within GraphRAG. These retrievers are particularly adept at handling complex graph data structures\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#graph-neural-networks-gnn","title":"Graph Neural Networks (GNN)","text":"<p>GNNs are a class of deep learning models designed to handle graph-structured data. They operate by aggregating information from a node's neighbors to generate node embeddings, which can then be used for various tasks, such as node classification or graph-based retrieval. In GraphRAG, GNNs are often employed to encode graph data and guide retrieval processes\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#graph-retrieval-augmented-generation-graphrag","title":"Graph Retrieval-Augmented Generation (GraphRAG)","text":"<p>A methodology that enhances Retrieval-Augmented Generation (RAG) by leveraging graph data structures. GraphRAG retrieves elements such as nodes, triples, paths, or subgraphs from a pre-constructed graph database, allowing for more precise and context-aware generation by considering the interconnections between texts\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#graph-based-indexing","title":"Graph-Based Indexing","text":"<p>Graph-Based Indexing is a method for organizing and indexing graph data to optimize retrieval operations. This involves mapping node and edge properties, establishing pointers between connected nodes, and organizing the data to support efficient traversal and retrieval. It plays a crucial role in enhancing query efficiency in GraphRAG\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#graph-enhanced-generation","title":"Graph-Enhanced Generation","text":"<p>Abbreviation: G-Generation</p> <p>The phase in GraphRAG where meaningful outputs or responses are synthesized based on the retrieved graph data. This stage involves taking the query, retrieved graph elements, and an optional prompt as inputs to generate a contextually relevant and accurate response\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#graphrag-step","title":"GraphRAG Step","text":"<p>The key steps in a GraphRAG process, as outlined in the Summary paper, are:</p> <ol> <li>Graph-Based Indexing (G-Indexing)</li> </ol> <p>This initial phase involves constructing or identifying a graph database that aligns with the downstream tasks. The indexing process typically includes mapping node and edge properties, establishing pointers between connected nodes, and organizing the data to support fast traversal and retrieval operations. The quality of this step directly impacts the efficiency and effectiveness of the retrieval process.</p> <ol> <li>Graph-Guided Retrieval (G-Retrieval)</li> </ol> <p>Following the indexing phase, the retrieval phase focuses on extracting relevant information from the graph database based on the user query. This involves selecting the most pertinent graph elements, such as entities, triplets, paths, or subgraphs, using retrieval algorithms. The goal is to find the graph elements that best match the query and support the next stage of generation.</p> <ol> <li>Graph-Enhanced Generation (G-Generation)</li> </ol> <p>In the final phase, the retrieved graph elements are used to generate meaningful responses or outputs. This stage involves taking the query, the retrieved graph elements, and an optional prompt as inputs for a generation model, which produces the final response. The process is enhanced by the structural and relational knowledge embedded in the graph data.</p> <p>These steps outline the comprehensive workflow of GraphRAG, emphasizing the importance of each phase in achieving accurate, contextually relevant responses based on graph-structured data\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#hallucination","title":"Hallucination","text":"<p>Hallucination in natural language processing refers to the generation of content by a model that is factually incorrect, nonsensical, or not supported by the input data. This occurs when the model produces information that appears coherent but is either fabricated or irrelevant to the context.</p> <p>For example, a language model might hallucinate by generating a detailed but entirely fictitious historical event when asked about a specific time period.</p>"},{"location":"glossary/#hybrid-granularities","title":"Hybrid Granularities","text":"<p>Hybrid Granularities involve retrieving graph data at multiple levels of detail, such as nodes, triplets, paths, and subgraphs, to capture both fine-grained and broad contextual information. This approach enhances the relevance and comprehensiveness of the retrieved data, making it useful for complex queries within GraphRAG \u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#knowledge-base-question-answering","title":"Knowledge Base Question Answering","text":"<p>Abbreviation: KBQA</p> <p>A task in natural language processing focused on answering user queries based on external knowledge bases. KBQA methods are typically categorized into Information Retrieval (IR)-based methods, which retrieve information from knowledge graphs, and Semantic Parsing (SP)-based methods, which generate logical forms to query knowledge bases\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#knowledge-merging","title":"Knowledge Merging","text":"<p>A post-retrieval enhancement strategy in GraphRAG that involves combining multiple pieces of retrieved information to provide a comprehensive view. This process assists in consolidating knowledge, improving the relevance and completeness of the final retrieved results\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#knowledge-pruning","title":"Knowledge Pruning","text":"<p>A technique used after initial retrieval to refine and improve the quality of the retrieved information by removing redundant or less relevant data. It aims to ensure that the final results are highly pertinent to the user's query, enhancing the relevance of the information presented\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#lm-based-retriever","title":"LM-based Retriever","text":"<p>LM-based retrievers leverage the natural language processing capabilities of language models to retrieve relevant graph data based on text queries. These models are particularly effective for interpreting and processing diverse queries in GraphRAG. They are used to generate reasoning paths and assist in retrieving the most relevant graph elements\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#language-models","title":"Language Models","text":"<p>Abbreviation: LMs Language models (LMs) are advanced AI models designed to understand and generate human language. They are mainly classified into two types: discriminative models, like BERT and RoBERTa, which focus on tasks such as text classification, and generative models, like GPT-3 and GPT-4, which are used for tasks like machine translation and text generation\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#leiden-algorithm","title":"Leiden Algorithm","text":"<p>The Leiden Algorithm is an iterative method used for community detection in large networks. It improves upon the Louvain algorithm by guaranteeing well-connected communities, reducing the likelihood of disconnected or fragmented clusters within the detected communities.</p> <p>For example, the Leiden Algorithm is often applied in graph analysis to identify groups of nodes that are more densely connected internally, such as in social network analysis to find clusters of users with similar interests.</p> <p>Leiden algorithm, are used to partition the graph into communities of nodes with stronger connections to one another than to other nodes in the graph. This helps in grouping closely related entities together</p>"},{"location":"glossary/#non-parametric-retriever","title":"Non-parametric Retriever","text":"<p>Non-parametric retrievers are based on heuristic rules or traditional graph search algorithms. They do not rely on deep-learning models, making them efficient for retrieval tasks that require high speed. These methods typically involve pre-processing steps like entity linking and are used in GraphRAG for tasks like subgraph extraction\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#paths","title":"Paths","text":"<p>Paths represent sequences of relationships between entities within a graph. In GraphRAG, retrieving paths enhances contextual understanding and reasoning capabilities by capturing complex relationships and dependencies\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#query-decomposition","title":"Query Decomposition","text":"<p>A technique that breaks down the original user query into smaller, more specific sub-queries. Each sub-query focuses on a particular aspect of the original query, making it easier to retrieve pertinent information and reduce the complexity of language queries\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#query-expansion","title":"Query Expansion","text":"<p>A pre-processing technique used in query enhancement to improve search results by supplementing or refining the original query with additional relevant terms or concepts. It aims to capture lexical variations and expand the information content of the query\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#query-focused-summarization","title":"Query-Focused Summarization","text":"<p>Abbreviation: QFS</p> <p>A task in natural language processing where the goal is to generate a summary of a document or set of documents that is focused on a specific query. GraphRAG enhances QFS by retrieving relevant subgraphs that capture the broader context and interconnections within the graph structure\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<p>Abbreviation: RAG Retrieval-Augmented Generation is a methodology that integrates external knowledge retrieval with the generation process of language models. By dynamically querying a large text corpus, RAG enhances the contextual depth, factual accuracy, and relevance of the content generated by language models. It addresses challenges such as hallucinations, lack of domain-specific knowledge, and outdated information by incorporating relevant factual knowledge into the responses\u200b(GraphRAG-Survey)\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#subgraphs","title":"Subgraphs","text":"<p>In GraphRAG, subgraphs refer to comprehensive subsets of a graph that encapsulate relational contexts within a larger structure. Retrieving subgraphs allows for capturing complex patterns, sequences, and dependencies, which are crucial for a deeper understanding of semantic connections in graph-based tasks\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#synset","title":"Synset","text":"<p>A group of words or phrases that have similar meaning.</p> <p>For example, the words \"baby\", \"infant\" and \"child\" may be consider rough synonyms.</p>"},{"location":"glossary/#text-attributed-graphs","title":"Text-Attributed Graphs","text":"<p>Abbreviation: TAGs</p> <p>A universal format for graph data used in GraphRAG, where nodes and edges are associated with textual attributes. TAGs can represent knowledge graphs and other graph structures, enabling the retrieval of complex relational knowledge for use in generation tasks\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#triplets","title":"Triplets","text":"<p>Triplets in graph databases consist of entities and their relationships, represented as subject-predicate-object tuples. This structured representation is used in GraphRAG to retrieve and understand relational data within a graph\u200b(GraphRAG-Survey).</p>"},{"location":"glossary/#vector-indexing","title":"Vector Indexing","text":"<p>Vector Indexing involves converting graph data into vector representations to facilitate fast retrieval. This is particularly useful for entity linking and rapid query processing, where vector search algorithms like Locality Sensitive Hashing (LSH) are applied. It enables efficient searches and helps maintain structural information during retrieval\u200b(GraphRAG-Survey).</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>The language of data science, machine learning, and AI is Python.  So we wanted to create a site that could be easily understood by Python developers.  Mkdocs is the leading documentation tool used by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Graph RAG References","text":"<ol> <li>Graph Retrieval-Augmented Generation: A Survey</li> <li>From Local to Global: A Graph RAG Approach to Query-Focused Summarization</li> </ol>"},{"location":"case-studies/","title":"GraphRAG Case Studies","text":""},{"location":"case-studies/#ollama-on-a-local-gpu","title":"OLlama on a local GPU","text":""},{"location":"case-studies/ollama-local/","title":"Ollama on an Local GPU","text":"<pre><code>ollama list\n</code></pre> <pre><code>NAME                ID              SIZE    MODIFIED    \nphi3:latest         4f2222927938    2.2 GB  2 hours ago \nllama3.1:latest     91ab477bec9d    4.7 GB  8 days ago  \ncodellama:latest    8fdf8f752f6e    3.8 GB  8 days ago  \nphi3:mini           4f2222927938    2.2 GB  8 days ago \n</code></pre>"},{"location":"case-studies/ollama-local/#sample-python-code","title":"Sample Python Code","text":"<pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_ollama.llms import OllamaLLM\nimport torch\n\n# clean up any models in RAM\ntorch.cuda.empty_cache() \n\ntemplate = \"\"\"\nWhy does LLM infernce take up so much GPU RAM?\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\nmodel = OllamaLLM(model=\"phi3:mini\")\nchain = prompt | model\n\nchain.invoke({\"question\": \"What is LangChain?\"})\n</code></pre>"},{"location":"concepts/","title":"GraphRAG Concepts","text":""},{"location":"concepts/#beginning-concepts","title":"Beginning Concepts","text":"<ol> <li>LLMs</li> <li>Graph databases</li> <li>Vector stores</li> <li>RAG</li> <li>GraphRAG</li> <li>Prompts</li> <li>Ollama</li> </ol>"},{"location":"concepts/#advanced-concepts","title":"Advanced Concepts","text":"<ol> <li>NLP Pipelines</li> <li>Entity Extraction</li> <li>Concept Extraction</li> <li>Concept Networks</li> <li>Wordnet</li> </ol>"},{"location":"concepts/concept-extraction/","title":"Concept Extraction","text":"<p>After extracting concrete POLE (Person, Organization, Location, and Event) entities, and abstract concepts in a typical NLP pipeline, the next step might be to extract finer grain information:</p> <ol> <li> <p>Relationships Between Entities: Identifying and categorizing the relationships between the extracted entities, such as connections between people, affiliations with organizations, or events happening at specific locations.</p> </li> <li> <p>Temporal Information: Extracting dates, times, and durations associated with events or actions, which helps in understanding the timeline of events and activities.</p> </li> <li> <p>Sentiment and Opinion: Analyzing the sentiment or opinion expressed in the text related to the entities or events, which provides context on how subjects or events are perceived.</p> </li> <li> <p>Action/Verb Extraction: Identifying actions or verbs related to the entities, which helps in understanding what is being done by or to the entities mentioned.</p> </li> <li> <p>Attributes and Qualifiers: Extracting descriptive information or attributes associated with entities, such as titles, roles, or any other qualifiers that add more detail to the entity.</p> </li> <li> <p>Coreference Resolution: Determining when different expressions in the text refer to the same entity (e.g., \"John\" and \"he\") to improve the coherence and accuracy of the extracted information.</p> </li> <li> <p>Semantic Roles: Identifying the roles entities play in relation to actions or events (e.g., agent, patient, instrument), which contributes to a deeper understanding of sentence structure and meaning.</p> </li> <li> <p>Causal Relationships: Identifying cause-and-effect relationships between events, which can help in constructing a more comprehensive understanding of the narrative or sequence of events.</p> </li> </ol> <p>These additional concepts help in building a richer, more structured representation of the information in the text, enabling more sophisticated downstream applications such as knowledge graph construction, question answering, or process mining.</p> <p>Identifying abstract concepts in an NLP pipeline is a more complex task compared to extracting more concrete entities like POLE (Person, Organization, Location, Event). Abstract concepts include ideas, emotions, theories, and other intangible entities that don't have a physical presence but are important for understanding the deeper meaning of the text. Here's how they are typically identified:</p>"},{"location":"concepts/concept-extraction/#1-semantic-analysis","title":"1. Semantic Analysis:","text":"<ul> <li>Word Embeddings: Abstract concepts are often identified by analyzing the semantic similarity between words using techniques like word embeddings (e.g., Word2Vec, GloVe). These embeddings capture the context in which words appear, helping to group abstract concepts together based on their meanings.</li> <li>Contextual Models: Transformer-based models like BERT, GPT, or others analyze words within their specific context, allowing the identification of abstract concepts based on how they are used in a sentence. For example, the word \"freedom\" might be recognized as an abstract concept depending on its context in a sentence.</li> </ul>"},{"location":"concepts/concept-extraction/#3-pattern-recognition","title":"3. Pattern Recognition:","text":"<ul> <li>Linguistic Patterns: Certain syntactic or grammatical patterns in text often indicate the presence of abstract concepts. For example, phrases like \"the concept of,\" \"the idea of,\" or \"the theory that\" often precede abstract nouns.</li> <li>Dependency Parsing: Analyzing the syntactic structure of sentences through dependency parsing helps in identifying abstract concepts based on their roles in a sentence.</li> </ul>"},{"location":"concepts/concept-extraction/#4-sentiment-and-emotion-analysis","title":"4. Sentiment and Emotion Analysis:","text":"<ul> <li>Emotion Detection: Abstract concepts related to emotions (like \"happiness,\" \"anger,\" or \"love\") can be identified through emotion detection models that analyze the emotional tone of the text.</li> <li>Subjectivity Analysis: Abstract concepts often occur in subjective statements. Tools that differentiate between objective and subjective content can help isolate abstract ideas.</li> </ul>"},{"location":"concepts/concept-extraction/#5-named-entity-recognition-ner-extensions","title":"5. Named Entity Recognition (NER) Extensions:","text":"<ul> <li>Custom NER Models: While traditional NER models focus on concrete entities, custom models can be trained to recognize abstract entities, particularly in specialized domains like law, philosophy, or psychology.</li> </ul>"},{"location":"concepts/concept-extraction/#6-topic-modeling","title":"6. Topic Modeling:","text":"<ul> <li>Latent Dirichlet Allocation (LDA): Topic modeling techniques like LDA can help in identifying recurring abstract themes or concepts within large text corpora by clustering words that frequently appear together.</li> </ul>"},{"location":"concepts/concept-extraction/#7-human-in-the-loop-approaches","title":"7. Human-in-the-Loop Approaches:","text":"<ul> <li>Annotation and Supervision: Since abstract concepts can be highly domain-specific, human annotators often manually label these concepts in text. These labels are then used to train supervised machine learning models that can identify similar concepts in new texts.</li> </ul>"},{"location":"concepts/concept-extraction/#examples-of-abstract-concepts","title":"Examples of Abstract Concepts:","text":"<ul> <li>Emotions: happiness, sorrow, anger</li> <li>Qualities: honesty, bravery, intelligence</li> <li>Theories: evolution, relativity</li> <li>Values: freedom, justice, equality</li> <li>States: peace, chaos, order</li> </ul> <p>Identifying abstract concepts requires a combination of linguistic, semantic, and contextual analyses, often customized to the specific needs of the application.</p> <p>4o</p>"},{"location":"concepts/concept-extraction/#you-said","title":"You said:","text":"<p>What are some benchmarks that measure the ability of an NLP pipeline to find abstract concepts?</p>"},{"location":"concepts/concept-extraction/#chatgpt-said","title":"ChatGPT said:","text":"<p>ChatGPT</p> <p>Evaluating the ability of an NLP pipeline to find abstract concepts is challenging due to the complexity and subjectivity involved in recognizing these concepts. However, several benchmarks and datasets are used in the NLP community to assess this ability, often as part of broader evaluations of semantic understanding, sentiment analysis, or contextual comprehension. Here are some notable benchmarks and evaluation frameworks:</p>"},{"location":"concepts/concept-extraction/#1-winograd-schema-challenge-wsc","title":"1. Winograd Schema Challenge (WSC)","text":"<ul> <li>Purpose: This challenge is designed to test a system's ability to understand commonsense reasoning and disambiguate pronouns based on context. While it doesn't directly focus on abstract concepts, the ability to correctly resolve pronouns often requires understanding abstract ideas and relationships.</li> <li>Example: \"The trophy doesn't fit in the brown suitcase because it is too large.\" Understanding \"large\" as an abstract concept related to size is key to resolving what \"it\" refers to.</li> </ul>"},{"location":"concepts/concept-extraction/#2-superglue","title":"2. SuperGLUE","text":"<ul> <li>Purpose: SuperGLUE is an NLP benchmark designed to evaluate a wide range of language understanding tasks, including tasks that involve abstract reasoning, inference, and recognition of complex semantic relationships.</li> <li>Tasks Related to Abstract Concepts:<ul> <li>CommitmentBank (CB): Requires systems to judge the degree to which a statement is true based on context, often involving abstract reasoning.</li> <li>Multi-Sentence Reading Comprehension (ReCoRD): Focuses on reading comprehension with a particular emphasis on resolving references that may involve abstract concepts.</li> <li>Winograd Schema (WSC): Included in SuperGLUE as a challenge task.</li> </ul> </li> </ul>"},{"location":"concepts/concept-extraction/#3-conceptnet-and-atomic","title":"3. ConceptNet and ATOMIC","text":"<ul> <li>Purpose: ConceptNet and ATOMIC are knowledge graphs focused on commonsense knowledge. They include abstract concepts and are often used to evaluate the ability of NLP systems to understand and reason about abstract ideas.</li> <li>Evaluation: NLP systems are evaluated on their ability to predict relationships or generate plausible statements involving abstract concepts like \"cause,\" \"effect,\" or \"intent.\"</li> </ul>"},{"location":"concepts/concept-extraction/#4-semeval-semantic-evaluation","title":"4. SemEval (Semantic Evaluation)","text":"<ul> <li>Purpose: The SemEval competition includes various tasks that often involve understanding abstract concepts, particularly in the context of sentiment analysis, emotion recognition, and metaphor detection.</li> <li>Tasks Related to Abstract Concepts:<ul> <li>Sentiment Analysis: Understanding abstract concepts related to sentiment (e.g., love, hate).</li> <li>Metaphor Detection: Identifying metaphors requires recognizing abstract mappings between different concepts.</li> </ul> </li> </ul>"},{"location":"concepts/concept-extraction/#5-abstract-meaning-representation-amr-benchmark","title":"5. Abstract Meaning Representation (AMR) Benchmark","text":"<ul> <li>Purpose: AMR is a semantic representation framework that encodes the meaning of a sentence in a graph structure, abstracting away from the surface form. Evaluating NLP systems on AMR parsing benchmarks measures their ability to capture the abstract meaning of sentences.</li> <li>Datasets: The AMR corpus is used to benchmark systems that translate natural language sentences into their corresponding abstract meaning representations.</li> </ul>"},{"location":"concepts/concept-extraction/#6-metaphor-identification-benchmark-vua-metaphor-corpus","title":"6. Metaphor Identification Benchmark (VUA Metaphor Corpus)","text":"<ul> <li>Purpose: This corpus is specifically designed to evaluate a system's ability to detect metaphors in text. Since metaphors often involve abstract concepts, this benchmark is relevant for evaluating the recognition of abstract ideas.</li> <li>Evaluation: Systems are scored on their accuracy in identifying metaphorical expressions, which are inherently abstract.</li> </ul>"},{"location":"concepts/concept-extraction/#7-winogender-schema","title":"7. Winogender Schema","text":"<ul> <li>Purpose: This benchmark is an extension of the Winograd Schema Challenge, designed to measure gender bias in coreference resolution. It also tests the system's ability to understand abstract gender-related concepts in various contexts.</li> <li>Evaluation: The focus is on how well systems can resolve pronouns while avoiding gender bias, which often involves understanding abstract notions of gender roles and identities.</li> </ul>"},{"location":"concepts/concept-extraction/#8-commonsenseqa","title":"8. CommonSenseQA","text":"<ul> <li>Purpose: CommonSenseQA is a multiple-choice question-answering dataset that requires systems to answer questions based on commonsense knowledge, which often involves abstract reasoning.</li> <li>Evaluation: Systems are evaluated on their accuracy in selecting the correct answer, which requires understanding abstract relationships and concepts.</li> </ul>"},{"location":"concepts/concept-extraction/#9-story-cloze-test-sct","title":"9. Story Cloze Test (SCT)","text":"<ul> <li>Purpose: This benchmark evaluates the system's ability to predict the correct ending to a story based on abstract reasoning and understanding of the narrative.</li> <li>Evaluation: Performance is measured by how accurately the system can choose the most plausible ending, which often involves abstract concepts like intention, emotion, and causality.</li> </ul>"},{"location":"concepts/concept-extraction/#10-abstract-noun-classification-tasks","title":"10. Abstract Noun Classification Tasks","text":"<ul> <li>Purpose: Specific tasks or datasets may be designed to classify nouns as abstract or concrete, testing the system's ability to differentiate and recognize abstract concepts.</li> <li>Evaluation: Accuracy in classifying abstract nouns or phrases as opposed to concrete ones.</li> </ul> <p>These benchmarks help assess an NLP pipeline's performance in identifying and reasoning about abstract concepts, often as part of broader semantic understanding or commonsense reasoning tasks. The complexity of abstract concepts means that these evaluations usually require sophisticated models that can capture deep contextual and semantic nuances.</p>"},{"location":"concepts/concept-networks/","title":"Concept Networks","text":"<p>A Concept Network or Concept Graph is a structured representation of related terms and concepts that captures the relationships between them, such as synonyms, alternate terms, abbreviations, broader terms, and narrower terms. It serves as a semantic map that links various expressions of an idea or entity, enabling the system to recognize and interpret different linguistic variations of the same concept.</p>"},{"location":"concepts/concept-networks/#key-components-of-a-concept-network","title":"Key Components of a Concept Network:","text":"<ol> <li>Synonyms: Words or phrases that have the same or nearly the same meaning (e.g., \"car\" and \"automobile\").  A group of words that have the same mean is often referred to as a [synset].</li> <li>Alternate Terms: Different expressions or names that refer to the same concept (e.g., \"AI\" and \"Artificial Intelligence\").</li> <li>Abbreviations: Shortened forms of terms that represent the same concept (e.g., \"USA\" for \"United States of America\").</li> <li>Broader Terms: General categories that encompass the concept (e.g., \"vehicle\" as a broader term for \"car\").</li> <li>Narrower Terms: More specific instances or subcategories of the concept (e.g., \"sedan\" as a narrower term for \"car\").</li> <li>Related Terms: Concepts that are not synonyms but are associated with the main concept (e.g., \"engine\" related to \"car\").</li> <li>Antonymys: Terms that represent the opposite meaning of a term.  When combined with a negation, an opposite that imply the opposite of concept. </li> </ol>"},{"location":"concepts/concept-networks/#purpose-and-importance","title":"Purpose and Importance:","text":"<ul> <li>Semantic Understanding: Concept Networks enhance a system's ability to understand and interpret text by recognizing different ways a concept can be expressed.</li> <li>Information Retrieval: They improve search and retrieval processes by linking queries to relevant terms, even if the exact term is not used.</li> <li>Text Analysis: Concept Networks are crucial in tasks like text classification, clustering, and summarization, where understanding the nuances of language is key.</li> </ul> <p>In summary, a Concept Network in NLP is a semantic framework that captures the various ways a concept can be expressed, facilitating better understanding, retrieval, and analysis of textual information.</p> <ul> <li>Ontology or Knowledge Graphs: Pre-existing ontologies or knowledge graphs can be used to map words and phrases to abstract concepts. For example, the word \"justice\" might be linked to related concepts like \"fairness\" or \"law\" in an ontology, enabling the extraction of these abstract ideas.</li> <li>Synonym and Hypernym Identification: Using lexical databases like WordNet, abstract concepts can be identified by recognizing synonyms and hypernyms (more general terms) that point to a higher-level concept.</li> </ul>"},{"location":"concepts/entity-extraction/","title":"Entity Extraction","text":"<p>Entity Extraction is the process of identifying and categorizing key pieces of information (entities) from text, such as names of people, organizations, locations, dates, and other specific concepts. These entities are often represented by proper nouns or specific phrases that are crucial to understanding the text's meaning.</p> <p>Entity extraction in the GraphRAG process plays a critical role in identifying and linking entities mentioned in the input query to corresponding entities in a knowledge graph. This step is essential for effectively retrieving relevant information and establishing connections between the textual data and the structured graph data.</p> <p>Entity extraction is often considered a pre-processing step in GraphRAG. It involves identifying key entities within the user's query and linking them to their corresponding nodes in the graph database. This ensures that the retrieval process can focus on the correct parts of the graph, thereby enhancing the relevance and accuracy of the retrieved information.</p> <p>It is particularly crucial in the Graph-Guided Retrieval phase, where entities identified in the text are mapped to their graph representations. This mapping allows the retrieval algorithms to more accurately extract the relevant subgraphs, nodes, or paths that correspond to the query.</p> <p>Entity Extraction typically occurs at the beginning of the Graph-Guided Retrieval** phase. After the graph-based indexing phase, where the graph is prepared and indexed for efficient querying, entity extraction takes place to identify which parts of the graph are relevant to the query. This step directly influences the subsequent retrieval processes, ensuring that the retrieval is focused and efficient.</p> <p>Entity extraction is thus a foundational step in aligning the unstructured text query with the structured graph data, enabling the GraphRAG system to retrieve and generate more contextually relevant responses\u200b(GraphRAG-Survey)\u200b(GraphRAG-Survey).</p>"},{"location":"concepts/entity-extraction/#why-entity-extraction-is-important-in-the-graphrag-algorithm","title":"Why Entity Extraction is Important in the GraphRAG Algorithm:","text":""},{"location":"concepts/entity-extraction/#building-knowledge-graphs","title":"Building Knowledge Graphs","text":"<p>Entity extraction is essential for constructing knowledge graphs, which are structured representations of information. By identifying entities, the GraphRAG algorithm can map these entities as nodes in the graph, forming the foundation for connecting and linking various pieces of information.</p>"},{"location":"concepts/entity-extraction/#enhancing-information-retrieval","title":"Enhancing Information Retrieval","text":"<p>In the context of the GraphRAG (Graph Retrieval-Augmented Generation) algorithm, entity extraction allows for precise retrieval of relevant information from a knowledge graph. When a user query is mapped to specific entities, the algorithm can more effectively navigate the graph to find accurate and contextually relevant information.</p>"},{"location":"concepts/entity-extraction/#enabling-contextual-understanding","title":"Enabling Contextual Understanding:**","text":"<p>By extracting entities, the algorithm can understand the context of a query or document. This contextual understanding is critical in generating accurate and relevant responses, as the algorithm can link the query to the right nodes and paths in the graph.</p>"},{"location":"concepts/entity-extraction/#improving-query-mapping-and-response-generation","title":"Improving Query Mapping and Response Generation:**","text":"<p>Entity extraction helps in accurately mapping user queries to the graph's structure. This mapping is crucial for the GraphRAG algorithm to generate responses that are informed by the specific entities and their relationships within the knowledge graph.</p>"},{"location":"concepts/entity-extraction/#supporting-complex-relationships-and-reasoning","title":"Supporting Complex Relationships and Reasoning:**","text":"<p>GraphRAG leverages the connections between entities in the knowledge graph to perform reasoning tasks, such as inferring new information or understanding complex relationships. Accurate entity extraction ensures that the graph is populated with the right entities, allowing for more sophisticated reasoning and inference capabilities.</p>"},{"location":"concepts/entity-extraction/#picking-the-right-granularity-of-entities","title":"Picking the Right Granularity of Entities","text":"<p>Every business application might have different requirements for locating entities in a document.  A legal system might only care about finding the names of parties in a contract and the effective dates of the contract.  For these applications only simple entity extraction rules that find People, Organizations, Locations and Events (POLE) entities might be needed.  This makes the entity extraction pipeline fast and low cost.</p> <p>Other applications want to understand more nuanced concepts in a document such as overall tone, development of ideas, analysis of assignments or commitments.  These tools need better granularity of extracting both abstract concepts and the relationship between these concepts.</p> <p>In summary, Entity Extraction is a foundational step in the GraphRAG algorithm, enabling the creation of a rich, structured knowledge graph and facilitating accurate, context-aware retrieval and generation of information.</p>"},{"location":"concepts/graph-algorithms/","title":"Graph Algorithms","text":"<p>Graph algorithms are a set of computational procedures designed to analyze the relationships and structures within graphs. A graph is a data structure consisting of nodes (or vertices) and edges (or links) that represent connections between these nodes. Graph algorithms are used to solve problems related to graph theory, such as finding the shortest path between nodes, detecting cycles, identifying clusters or communities, and more.</p>"},{"location":"concepts/graph-algorithms/#types-of-graph-algorithms","title":"Types of Graph Algorithms","text":"<p>There are various types of graph algorithms, each serving different purposes:</p> <ul> <li>Traversal Algorithms: These algorithms explore the nodes and edges of a graph. Examples include Depth-First Search (DFS) and Breadth-First Search (BFS).</li> <li>Shortest Path Algorithms: These algorithms find the shortest path between two nodes. Examples include Dijkstra's algorithm and the A* algorithm.</li> <li>Connectivity Algorithms: These identify connected components within a graph, such as Tarjan's algorithm for finding strongly connected components.</li> <li>Centrality Algorithms: These measure the importance of a node within a graph, with examples including Betweenness Centrality and PageRank.</li> <li>Community Detection Algorithms: These algorithms identify clusters or communities within a graph. Examples include the Girvan-Newman algorithm, Louvain method, and Label Propagation.</li> <li>Graph Matching and Similarity Algorithms: These identify similarities or matchings between different graphs or subgraphs.</li> </ul> <p>2. Graph Algorithms in GraphRAG </p>"},{"location":"concepts/graph-algorithms/#21-what-is-graphrag","title":"2.1 What is GraphRAG?","text":"<p>GraphRAG (Graph Retrieval-Augmented Generation) is an advanced system that combines graph-based knowledge representation with retrieval-augmented generation techniques to enhance the capabilities of AI models, particularly in natural language processing (NLP) tasks. GraphRAG leverages knowledge graphs---structured representations of interconnected concepts---to retrieve relevant information and generate accurate, context-aware responses.</p>"},{"location":"concepts/graph-algorithms/#22-how-graphrag-works","title":"2.2 How GraphRAG Works","text":"<p>GraphRAG integrates two main components:</p> <ul> <li> <p>Graph-Based Knowledge Retrieval: This component uses a knowledge graph to store and retrieve information. When a user query is received, the system maps the query to relevant nodes (concepts) and retrieves connected nodes and relationships that provide context and detailed information.</p> </li> <li> <p>Generation Component: The retrieved information is then used to generate a response that is informed by the structured knowledge in the graph, making the output more accurate and contextually relevant.</p> </li> </ul>"},{"location":"concepts/graph-algorithms/#3-role-of-graph-algorithms-in-graphrag","title":"3. Role of Graph Algorithms in GraphRAG","text":""},{"location":"concepts/graph-algorithms/#31-community-detection-in-graphrag","title":"3.1 Community Detection in GraphRAG","text":"<p>Community detection is a crucial aspect of graph algorithms used within the GraphRAG system. In the context of GraphRAG, communities are clusters of concepts that are more densely connected to each other than to the rest of the graph. Detecting these communities helps the system understand and organize concepts into meaningful groups, facilitating more effective retrieval and response generation.</p>"},{"location":"concepts/graph-algorithms/#32-how-community-detection-works","title":"3.2 How Community Detection Works","text":"<p>Community detection algorithms in GraphRAG operate by analyzing the structure of the knowledge graph to identify groups of nodes that are closely related. Here's how it works:</p> <ul> <li> <p>Graph Construction: The knowledge graph is first constructed with nodes representing concepts and edges representing relationships between these concepts.</p> </li> <li> <p>Community Detection Algorithms: Algorithms like Louvain or Label Propagation are applied to the graph to detect communities. These algorithms group nodes based on the density of connections between them.</p> <ul> <li> <p>Louvain Method: This is a popular algorithm for community detection that optimizes modularity, a measure of the strength of division of a network into communities. It iteratively merges nodes into communities and then merges communities into larger communities, effectively identifying clusters in the graph.</p> </li> <li> <p>Label Propagation: This algorithm works by assigning labels to each node and then updating the labels based on the majority of the neighbors' labels. Over time, nodes with similar labels form communities.</p> </li> </ul> </li> <li> <p>Community Analysis: Once communities are detected, the system can analyze these clusters to understand the relationships and contexts within them. For example, concepts related to \"machine learning\" might form a community, including nodes like \"neural networks,\" \"supervised learning,\" and \"reinforcement learning.\"</p> </li> </ul>"},{"location":"concepts/graph-algorithms/#33-applications-of-community-detection-in-graphrag","title":"3.3 Applications of Community Detection in GraphRAG","text":"<p>1. Conceptual Grouping: By identifying communities, GraphRAG can group related concepts together, making it easier to retrieve and generate contextually accurate information.</p> <p>2. Efficient Query Mapping: When a query is received, the system can quickly map it to the relevant community, narrowing down the search space and improving response time.</p> <p>3. Enhanced Information Retrieval: Communities help the system retrieve not just direct matches but also related concepts, providing a more comprehensive response to complex queries.</p> <p>4. Contextual Understanding: Communities provide context to concepts by showing how they relate to other concepts within the same group, improving the system's ability to generate nuanced and accurate responses.</p>"},{"location":"concepts/graph-algorithms/#4-benefits-of-using-graph-algorithms-in-graphrag","title":"4. Benefits of Using Graph Algorithms in GraphRAG","text":""},{"location":"concepts/graph-algorithms/#41-improved-accuracy","title":"4.1 Improved Accuracy","text":"<p>Graph algorithms, especially community detection, allow GraphRAG to better organize and retrieve information, leading to more accurate and relevant responses.</p>"},{"location":"concepts/graph-algorithms/#42-scalability","title":"4.2 Scalability","text":"<p>As knowledge graphs grow in size and complexity, graph algorithms help manage this complexity by identifying key structures (like communities), enabling the system to scale effectively.</p>"},{"location":"concepts/graph-algorithms/#43-context-aware-generation","title":"4.3 Context-Aware Generation","text":"<p>By leveraging the structure of the knowledge graph, GraphRAG can generate responses that are not only accurate but also rich in context, making the system more effective in understanding and responding to user queries.</p>"},{"location":"concepts/graph-algorithms/#5-conclusion","title":"5. Conclusion","text":"<p>Graph algorithms are integral to the functioning of the GraphRAG system, particularly in the detection of communities of concepts within knowledge graphs. By organizing concepts into meaningful clusters, GraphRAG can more efficiently retrieve relevant information and generate contextually appropriate responses. This enhances the system's overall ability to provide accurate, context-aware information, making it a powerful tool for natural language processing and information retrieval tasks.</p> <p>This document provides a comprehensive overview of graph algorithms and their application in the GraphRAG system, particularly in detecting communities of concepts, which is crucial for enhancing the system's performance in retrieving and generating information.</p>"},{"location":"concepts/ollama/","title":"Ollama","text":"<p>Ollama is a free open source tool that makes it easy to run large-language models on your local home consumer-grade GPUs with limited RAM.  Getting set up with Ollama is only a matter of running two UNIX shell commands:</p>"},{"location":"concepts/ollama/#ollama-setup","title":"Ollama Setup","text":"<p>The following line downloads a UNIX install.sh shell script and runs it:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <pre><code>ollama run llama3.1\n</code></pre> <p>You can also run other models like Microsoft's Phi3 Mini:</p> <pre><code>ollama run phi3\n</code></pre>"},{"location":"concepts/ollama/#why-is-ollama-so-amazing","title":"Why is Ollama so Amazing?","text":"<p>Ollama's ability to run larger models on a GPU with limited memory, like my 12GB NVIDIA GeForce 2080 Ti GPU, is largely due to the following optimizations and techniques:</p> <ol> <li>Quantization: Ollama often uses techniques like 4-bit or 8-bit quantization, which reduce the precision of the model weights. This significantly decreases the memory footprint without a substantial loss in model performance, allowing larger models to fit into smaller GPU memory.</li> </ol> <p>As of this writing, the default precision in Ollama is 4-bits.</p> <ol> <li> <p>Memory-Efficient Architectures: Some models are designed with efficiency in mind, using architectures that require less memory per parameter or leveraging sparse computations that only activate a portion of the model at a time.</p> </li> <li> <p>Layer-by-Layer Loading: Ollama can load and unload model layers or parts of the model dynamically, processing one layer at a time instead of keeping the entire model in memory simultaneously. This is a form of model sharding, where only the active parts of the model are in memory.</p> </li> <li> <p>Optimized Memory Management: Advanced memory management techniques, such as swapping parts of the model in and out of GPU memory or using CPU RAM as an overflow buffer, help in running larger models. Although this might introduce some latency, it allows larger models to operate on smaller GPUs.</p> </li> <li> <p>Custom Kernels: Ollama might use custom GPU kernels optimized for the specific hardware, which can maximize the utilization of available memory and compute resources.</p> </li> <li> <p>Efficient Batch Processing: By processing smaller batches or even single tokens at a time, Ollama can reduce the amount of memory needed during inference, allowing for larger models to be loaded and processed.</p> </li> </ol> <p>These techniques enable Ollama to run models that are technically larger than what the raw memory capacity of the GPU would traditionally allow, albeit with some trade-offs in terms of speed or latency.</p>"},{"location":"concepts/prompts/","title":"LLM Prompts used in GraphRAG","text":""},{"location":"concepts/prompts/#entity-extraction-prompts","title":"Entity Extraction Prompts","text":""},{"location":"concepts/prompts/#prompt-1","title":"Prompt #1","text":"<p><pre><code>Please analyze the following text and extract the entities related to people,\norganizations, locations, and events. For each entity, return the output in a\nJSON format where each entity is represented as a node, and relationships\nbetween entities are represented as edges. \nEnsure that the output follows this structure:\n</code></pre> <pre><code>{\n    \"nodes\": [\n        {\"id\": \"1\", \"label\": \"Person\", \"name\": \"John Doe\"},\n        {\"id\": \"2\", \"label\": \"Organization\", \"name\": \"ACME Corp\"},\n        {\"id\": \"3\", \"label\": \"Location\", \"name\": \"New York\"},\n        {\"id\": \"4\", \"label\": \"Event\", \"name\": \"Annual Conference 2024\"}\n    ],\n    \"edges\": [\n        {\"source\": \"1\", \"target\": \"2\", \"relationship\": \"works_at\"},\n        {\"source\": \"1\", \"target\": \"3\", \"relationship\": \"lives_in\"},\n        {\"source\": \"2\", \"target\": \"4\", \"relationship\": \"organizes\"},\n        {\"source\": \"3\", \"target\": \"4\", \"relationship\": \"location_of\"}\n    ]\n}\n</code></pre> <pre><code>Please make sure to include appropriate relationships between the entities wherever applicable.  Avoid duplication of edges.\n</code></pre></p>"},{"location":"concepts/prompts/#prompt-2","title":"Prompt #2","text":"<p>In this version, we only ask the LLM to extract only the most important entities and avoid duplication of relationships.</p> <pre><code>Please analyze the following text and extract the most important\nentities related to people, organizations, locations, and events.\nFor each entity, return the output in a JSON format where each entity \nis represented as a node, and relationships between entities are\nrepresented as edges.\n\nEnsure that the output follows this structure:\n</code></pre> <pre><code>{\n   \"nodes\": [\n       {\"id\": \"1\", \"label\": \"Person\", \"name\": \"John Doe\"},\n       {\"id\": \"2\", \"label\": \"Organization\", \"name\": \"ACME Corp\"},\n       {\"id\": \"3\", \"label\": \"Location\", \"name\": \"New York\"},\n       {\"id\": \"4\", \"label\": \"Event\", \"name\": \"Annual Conference 2024\"}\n   ],\n   \"edges\": [\n       {\"source\": \"1\", \"target\": \"2\", \"relationship\": \"works_at\"},\n       {\"source\": \"1\", \"target\": \"3\", \"relationship\": \"lives_in\"},\n       {\"source\": \"2\", \"target\": \"4\", \"relationship\": \"organizes\"},\n       {\"source\": \"3\", \"target\": \"4\", \"relationship\": \"location_of\"}\n   ]\n}\n</code></pre> <pre><code>Please make sure to include appropriate relationships between the entities wherever applicable.  Avoid duplication of edges.\n</code></pre>"},{"location":"concepts/wordnet/","title":"Wordnet","text":"<p>WordNet is a large, lexical database of the English language that groups words into sets of synonyms called synsets, each representing a distinct concept. </p> <p>These synsets are interconnected through various semantic relationships, such as synonymy, antonymy, hypernymy, hyponymy, meronymy, and holonymy, among others. WordNet organizes nouns, verbs, adjectives, and adverbs into a hierarchical structure that mirrors the conceptual relationships between words, making it a valuable resource for natural language processing (NLP) tasks such as word sense disambiguation, semantic analysis, and information retrieval.</p> <p>Developed at Princeton University, WordNet serves as both a dictionary and a thesaurus, providing comprehensive information about word meanings and their interconnections.</p> <p>In the WordNet system, various types of relationships link words (specifically, synsets, which are sets of synonyms representing a single concept) together. These relationships help in organizing the lexicon in a way that reflects semantic connections between concepts. Here are the primary types of work relationships in WordNet:</p>"},{"location":"concepts/wordnet/#1-synonymy-synsets","title":"1. Synonymy (Synsets)","text":"<ul> <li>Definition: A relationship where two or more words have the same or nearly the same meaning.</li> <li>Example: The words \"car\" and \"automobile\" are synonyms and belong to the same synset.</li> </ul>"},{"location":"concepts/wordnet/#2-antonymy","title":"2. Antonymy","text":"<ul> <li>Definition: A relationship where two words have opposite meanings.</li> <li>Example: \"Hot\" and \"cold\" are antonyms.</li> </ul>"},{"location":"concepts/wordnet/#3-hypernymy-is-a-relationship","title":"3. Hypernymy (Is-a Relationship)","text":"<ul> <li>Definition: A hierarchical relationship where one word is a more general term (hypernym) for another.</li> <li>Example: \"Vehicle\" is a hypernym of \"car.\"</li> </ul>"},{"location":"concepts/wordnet/#4-hyponymy","title":"4. Hyponymy","text":"<ul> <li>Definition: The reverse of hypernymy, where one word is a more specific term (hyponym) under a broader category.</li> <li>Example: \"Car\" is a hyponym of \"vehicle.\"</li> </ul>"},{"location":"concepts/wordnet/#5-meronymy-part-whole-relationship","title":"5. Meronymy (Part-Whole Relationship)","text":"<ul> <li>Definition: A relationship where one word denotes a part of something that the other word denotes.</li> <li>Example: \"Wheel\" is a meronym of \"car.\"</li> </ul>"},{"location":"concepts/wordnet/#6-holonymy","title":"6. Holonymy","text":"<ul> <li>Definition: The reverse of meronymy, where one word denotes a whole that contains the other word.</li> <li>Example: \"Car\" is a holonym of \"wheel.\"</li> </ul>"},{"location":"concepts/wordnet/#7-troponymy","title":"7. Troponymy","text":"<ul> <li>Definition: A relationship where one verb is a more specific manner of doing another verb.</li> <li>Example: \"Stroll\" is a troponym of \"walk.\"</li> </ul>"},{"location":"concepts/wordnet/#8-entailment","title":"8. Entailment","text":"<ul> <li>Definition: A relationship between verbs where the action of one verb logically implies the action of another.</li> <li>Example: \"Snore\" entails \"sleep.\"</li> </ul>"},{"location":"concepts/wordnet/#9-coordinate-terms-co-hyponyms","title":"9. Coordinate Terms (Co-Hyponyms)","text":"<ul> <li>Definition: Words that share the same hypernym and are thus related as siblings in the hierarchy.</li> <li>Example: \"Dog\" and \"cat\" are co-hyponyms under the hypernym \"animal.\"</li> </ul>"},{"location":"concepts/wordnet/#10-derivationally-related-forms","title":"10. Derivationally Related Forms","text":"<ul> <li>Definition: A relationship where words share the same root but are different parts of speech.</li> <li>Example: \"Run\" (verb) and \"runner\" (noun).</li> </ul>"},{"location":"concepts/wordnet/#11-pertainyms","title":"11. Pertainyms","text":"<ul> <li>Definition: A relationship where an adjective is derived from a noun and indicates a relationship of belonging or pertaining to.</li> <li>Example: \"Musical\" (adjective) is a pertainym of \"music\" (noun).</li> </ul> <p>These relationships in WordNet form a rich and interconnected structure that enables various applications in NLP, such as semantic similarity measurement, word sense disambiguation, and lexical database creation.</p>"}]}